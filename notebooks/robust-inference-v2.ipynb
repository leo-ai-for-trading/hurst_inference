{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48381efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "# Input:\n",
    "CSV_PATH = \"spy_all.csv\"               # or use concat_daily_csvs(...) below\n",
    "\n",
    "# Target windows and lags per window (must match between V and Psi):\n",
    "PARAMS_TARGET = [\n",
    "    {\"window\": 60,  \"N_lags\": 6},     # windows in number of bars (post alignment)\n",
    "    {\"window\": 120, \"N_lags\": 6},\n",
    "    {\"window\": 150, \"N_lags\": 4},\n",
    "]\n",
    "\n",
    "# Truncation / pattern:\n",
    "TRUNCATION_C = 5.0                     # robust truncation multiplier (MAD-based)\n",
    "USE_PATTERN = True                     # multiplicative intraday U-shape removal\n",
    "\n",
    "# GMM grid (open interval to avoid edges):\n",
    "H_MIN, H_MAX, H_MESH = 0.05, 0.45, 1e-3\n",
    "\n",
    "# Market RTH (for alignment):\n",
    "RTH_OPEN = (9, 30)                     # 09:30\n",
    "RTH_CLOSE = (16, 0)                    # 16:00\n",
    "EXCLUDE_DATES: set[pd.Timestamp] = set()  # add dates to exclude if desired\n",
    "\n",
    "# Monte Carlo (optional):\n",
    "DO_MC = False                          # set True to run MC study\n",
    "N_MC = 200\n",
    "SEED = 123\n",
    "FINE_DT_SECONDS = 0.5                  # fine grid for MC sim\n",
    "SAMPLE_DT_SECONDS = 5                  # post-subsample grid for MC\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities: I/O & alignment\n",
    "# =========================\n",
    "def concat_daily_csvs(data_dir: Path, out_path: Path) -> Path:\n",
    "    \"\"\"Concatenate spy_*.csv* files with columns DT, Price.\"\"\"\n",
    "    files = sorted([p for p in data_dir.glob(\"spy_*.csv*\") if p.is_file()])\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No files like 'spy_*.csv*' found in {data_dir}\")\n",
    "    parts = []\n",
    "    for p in files:\n",
    "        if p.suffix.lower() == \".icloud\":\n",
    "            print(f\"⏭️  Skipping placeholder: {p.name}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(p, usecols=[\"DT\", \"Price\"])\n",
    "            df[\"DT\"] = pd.to_datetime(df[\"DT\"], errors=\"coerce\")\n",
    "            df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"DT\", \"Price\"])\n",
    "            if not df.empty:\n",
    "                parts.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p.name}: {e}\")\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No readable CSVs after filtering placeholders/errors.\")\n",
    "    df_all = pd.concat(parts, ignore_index=True)\n",
    "    df_all = df_all.sort_values(\"DT\").drop_duplicates(subset=\"DT\", keep=\"last\").reset_index(drop=True)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_csv(out_path, index=False)\n",
    "    print(f\"[concat] Saved merged file with {len(df_all):,} rows → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def _align_one_df(df_in: pd.DataFrame, dt_sec: int, verbose: bool) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Align naive-ET dataframe to a strict RTH grid at dt_sec, with forward/backward fill.\"\"\"\n",
    "    df = df_in.dropna(subset=[\"DT\", \"Price\"]).copy()\n",
    "    df = df.sort_values(\"DT\")\n",
    "    df[\"date\"] = df[\"DT\"].dt.normalize()\n",
    "\n",
    "    n_kept, n_skipped = 0, 0\n",
    "    out = []\n",
    "    for d, g in df.groupby(\"date\", sort=True):\n",
    "        day = pd.Timestamp(d)\n",
    "        if day in EXCLUDE_DATES:\n",
    "            continue\n",
    "        start = day.replace(hour=RTH_OPEN[0], minute=RTH_OPEN[1], second=0)\n",
    "        end = day.replace(hour=RTH_CLOSE[0], minute=RTH_CLOSE[1], second=0)\n",
    "\n",
    "        gi = g.set_index(\"DT\").sort_index()\n",
    "        gi_buf = gi.loc[(gi.index >= start - pd.Timedelta(\"2min\")) & (gi.index <= end)]\n",
    "        if gi_buf.empty:\n",
    "            n_skipped += 1\n",
    "            continue\n",
    "\n",
    "        grid = pd.date_range(start, end, freq=f\"{dt_sec}S\")\n",
    "        gi2 = gi_buf.reindex(grid)\n",
    "        gi2[\"Price\"] = gi2[\"Price\"].ffill().bfill()\n",
    "\n",
    "        # keep only if at least one real RTH obs\n",
    "        if gi.loc[(gi.index >= start) & (gi.index <= end)].empty:\n",
    "            n_skipped += 1\n",
    "            continue\n",
    "\n",
    "        gi2 = gi2.rename_axis(\"DT\").reset_index()\n",
    "        gi2[\"date\"] = day\n",
    "        out.append(gi2[[\"DT\", \"date\", \"Price\"]])\n",
    "        n_kept += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[align] dt={dt_sec}s → kept days: {n_kept} | skipped: {n_skipped}\")\n",
    "    if not out:\n",
    "        return None\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "def load_and_align(path_csv: Path, dt_sec: int, verbose: bool = True) -> Optional[pd.DataFrame]:\n",
    "    raw = pd.read_csv(path_csv, parse_dates=[\"DT\"]).dropna(subset=[\"DT\", \"Price\"])\n",
    "\n",
    "    aligned = _align_one_df(raw, dt_sec, verbose)\n",
    "    if aligned is not None and not aligned.empty:\n",
    "        return aligned\n",
    "\n",
    "    # fallback: treat DT as UTC→New York\n",
    "    raw2 = raw.copy()\n",
    "    dt_parsed = pd.to_datetime(raw2[\"DT\"], utc=True, errors=\"coerce\")\n",
    "    raw2[\"DT\"] = dt_parsed.dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    aligned2 = _align_one_df(raw2, dt_sec, verbose)\n",
    "    if aligned2 is not None and not aligned2.empty:\n",
    "        print(\"[align] Interpreted timestamps as UTC and converted to New York.\")\n",
    "        return aligned2\n",
    "    return None\n",
    "\n",
    "\n",
    "def choose_dt_and_align(path_csv: Path, verbose: bool = True) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Try 5s; if not feasible, try 60s.\"\"\"\n",
    "    for cand in (5, 60):\n",
    "        aligned = load_and_align(path_csv, cand, verbose=verbose)\n",
    "        if aligned is not None and not aligned.empty:\n",
    "            if verbose:\n",
    "                print(f\"[align] Using dt_sec={cand}.\")\n",
    "            return aligned, cand\n",
    "    raise RuntimeError(\"No usable RTH days found at 5s or 60s.\")\n",
    "\n",
    "\n",
    "def to_daily_returns(aligned: pd.DataFrame) -> List[np.ndarray]:\n",
    "    \"\"\"Extract same-length daily log-returns arrays; keep the modal length.\"\"\"\n",
    "    rets = []\n",
    "    for _, g in aligned.groupby(\"date\", sort=True):\n",
    "        p = g[\"Price\"].to_numpy()\n",
    "        r = np.diff(np.log(p))\n",
    "        rets.append(r)\n",
    "    counts = pd.Series([len(x) for x in rets])\n",
    "    mode_len = int(counts.mode().iloc[0])\n",
    "    rets = [r for r in rets if len(r) == mode_len]\n",
    "    if len(rets) == 0:\n",
    "        raise RuntimeError(\"All days had inconsistent lengths after alignment.\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Volatility preprocessing\n",
    "# =========================\n",
    "def robust_truncate_returns(r_day: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"MAD-based clipping of returns to remove jumps/outliers (per day).\"\"\"\n",
    "    if r_day.size == 0:\n",
    "        return r_day\n",
    "    med = np.median(r_day)\n",
    "    mad = np.median(np.abs(r_day - med)) + 1e-16\n",
    "    sigma = mad / 0.67448975  # MAD -> std under normality\n",
    "    thresh = c * sigma + 1e-16\n",
    "    return np.clip(r_day, -thresh, thresh)\n",
    "\n",
    "\n",
    "def compute_intraday_pattern_sq(rets_days: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multiplicative U-shape pattern on per-bar variance proxy r^2.\n",
    "    Returns a length-T vector with mean 1.0 across bars.\n",
    "    \"\"\"\n",
    "    T = len(rets_days[0])\n",
    "    S = np.zeros(T, dtype=float)\n",
    "    cnt = 0\n",
    "    for r in rets_days:\n",
    "        if len(r) != T:\n",
    "            continue\n",
    "        S += r * r\n",
    "        cnt += 1\n",
    "    if cnt == 0:\n",
    "        raise RuntimeError(\"No days to compute pattern.\")\n",
    "    pattern = S / max(cnt, 1)\n",
    "    pattern = np.maximum(pattern, 1e-16)\n",
    "    return pattern / pattern.mean()\n",
    "\n",
    "\n",
    "def realized_variance_series_from_s2(s2: np.ndarray, K: int) -> np.ndarray:\n",
    "    \"\"\"Rolling sum over window K of per-bar variance proxy s2 (already truncated & pattern-normalized).\"\"\"\n",
    "    kernel = np.ones(K, dtype=float)\n",
    "    rv = np.convolve(s2, kernel, mode=\"valid\")  # length: len(s2)-K+1\n",
    "    return rv\n",
    "\n",
    "\n",
    "def volatility_increments(rv: np.ndarray, K: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Volatility increments at window K:\n",
    "    inc_t = RV[t+K] - RV[t], producing a series of length len(rv)-K.\n",
    "    \"\"\"\n",
    "    if len(rv) < 2 * K:\n",
    "        return np.array([], dtype=float)\n",
    "    return rv[K:] - rv[:-K]\n",
    "\n",
    "\n",
    "def sample_autocov(x: np.ndarray, L: int) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    γ_ℓ = 1/(n-ℓ) * sum_{t=0}^{n-ℓ-1} (x_t - μ)(x_{t+ℓ} - μ).\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return {ell: np.nan for ell in range(L + 1)}\n",
    "    mu = float(np.mean(x))\n",
    "    xc = x - mu\n",
    "    out: Dict[int, float] = {}\n",
    "    for ell in range(L + 1):\n",
    "        if ell >= n:\n",
    "            out[ell] = np.nan\n",
    "            continue\n",
    "        x1 = xc[: n - ell]\n",
    "        x2 = xc[ell:]\n",
    "        denom = max(n - ell, 1)\n",
    "        out[ell] = float(np.dot(x1, x2) / denom)\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_moment_vector_V(\n",
    "    days_returns: List[np.ndarray],\n",
    "    windows_and_lags: List[Dict[str, int]],\n",
    "    trunc_c: float,\n",
    "    use_pattern: bool,\n",
    ") -> Tuple[np.ndarray, Dict[str, Dict[int, float]]]:\n",
    "    \"\"\"\n",
    "    Build the stacked moment vector V across all requested windows,\n",
    "    using per-day truncated returns, pattern removal, volatility increments,\n",
    "    and FIRST-LAG correction. Also returns per-window autocovariances for inspection.\n",
    "    \"\"\"\n",
    "    # 1) Truncate per day\n",
    "    days_trunc = [robust_truncate_returns(r, trunc_c) for r in days_returns]\n",
    "\n",
    "    # 2) Pattern (U-shape) on per-bar variance proxy\n",
    "    if use_pattern:\n",
    "        pattern = compute_intraday_pattern_sq(days_trunc)\n",
    "    else:\n",
    "        T = len(days_trunc[0])\n",
    "        pattern = np.ones(T, dtype=float)\n",
    "\n",
    "    # 3) Build per-window increments concatenated across days, then autocovs\n",
    "    V_blocks: List[float] = []\n",
    "    ac_by_window: Dict[str, Dict[int, float]] = {}\n",
    "\n",
    "    for spec in windows_and_lags:\n",
    "        K = int(spec[\"window\"])\n",
    "        L = int(spec[\"N_lags\"])\n",
    "\n",
    "        # concatenate increments across days\n",
    "        inc_all = []\n",
    "        for r in days_trunc:\n",
    "            if len(r) != len(pattern):\n",
    "                continue\n",
    "            s2 = (r * r) / (pattern + 1e-16)  # multiplicative normalization to mean ~1\n",
    "            rv = realized_variance_series_from_s2(s2, K)\n",
    "            inc = volatility_increments(rv, K)\n",
    "            if inc.size > 0:\n",
    "                inc_all.append(inc)\n",
    "\n",
    "        if not inc_all:\n",
    "            raise RuntimeError(f\"No increments for window K={K}. Check data length or K.\")\n",
    "        y = np.concatenate(inc_all, axis=0)\n",
    "\n",
    "        # autocovariances at lags 0..L\n",
    "        gamma = sample_autocov(y, L)\n",
    "\n",
    "        # FIRST-LAG correction + stacking: [γ0+2γ1, γ2, γ3, ..., γL]\n",
    "        if L < 2:\n",
    "            raise ValueError(f\"N_lags must be at least 2 for first-lag correction (got {L}).\")\n",
    "        first = gamma[0] + 2.0 * gamma[1]\n",
    "        block = [first] + [gamma[ell] for ell in range(2, L + 1)]\n",
    "\n",
    "        V_blocks.extend(block)\n",
    "        ac_by_window[f\"K={K}\"] = gamma\n",
    "\n",
    "    V = np.array(V_blocks, dtype=float)\n",
    "    return V, ac_by_window\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model side: Ψ(H)\n",
    "# =========================\n",
    "def phi_H_l(ell: int, H: float) -> float:\n",
    "    \"\"\"\n",
    "    Five-point finite-difference operator Φ_H(ℓ) constructed from powers |·|^{2H}.\n",
    "    This symmetric discrete stencil captures the shape the estimator expects.\n",
    "    \"\"\"\n",
    "    # Define f(a) = |a|^{2H} with f(0)=0\n",
    "    def f(a: int) -> float:\n",
    "        if a == 0:\n",
    "            return 0.0\n",
    "        return abs(float(a)) ** (2.0 * H)\n",
    "\n",
    "    # 5-point symmetric combination (discrete 4th difference; scale not needed for GMM)\n",
    "    # Φ_H(ℓ) = f(ℓ+2) - 4 f(ℓ+1) + 6 f(ℓ) - 4 f(ℓ-1) + f(ℓ-2)\n",
    "    return f(ell + 2) - 4.0 * f(ell + 1) + 6.0 * f(ell) - 4.0 * f(ell - 1) + f(ell - 2)\n",
    "\n",
    "\n",
    "def build_Psi_vector(H: float, windows_and_lags: List[Dict[str, int]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build Ψ(H) as the concatenation across windows of:\n",
    "      [ Φ_H(0) + Φ_H(1), Φ_H(2), ..., Φ_H(L) ] * window^(2H)\n",
    "    \"\"\"\n",
    "    blocks: List[float] = []\n",
    "    for spec in windows_and_lags:\n",
    "        K = int(spec[\"window\"])\n",
    "        L = int(spec[\"N_lags\"])\n",
    "        if L < 2:\n",
    "            raise ValueError(\"Each window must have N_lags >= 2.\")\n",
    "        first = phi_H_l(0, H) + phi_H_l(1, H)\n",
    "        rest = [phi_H_l(ell, H) for ell in range(2, L + 1)]\n",
    "        block = [first] + rest\n",
    "        scale = (float(K) ** (2.0 * H))\n",
    "        blocks.extend([scale * b for b in block])\n",
    "    return np.array(blocks, dtype=float)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Estimation: GMM over H, closed-form R(H)\n",
    "# =========================\n",
    "@dataclass\n",
    "class GMMResult:\n",
    "    H: float\n",
    "    R: float\n",
    "    obj: float\n",
    "    H_grid: np.ndarray\n",
    "    obj_grid: np.ndarray\n",
    "\n",
    "\n",
    "def gmm_objective_with_closed_form_R(V: np.ndarray, Psi: np.ndarray, W: Optional[np.ndarray]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given V and Psi(H), and weight matrix W (default I), return:\n",
    "      R*(H) = argmin_R (V - R Psi)' W (V - R Psi) = (Psi' W V)/(Psi' W Psi)\n",
    "      F(H)  = (V - R* Psi)' W (V - R* Psi)\n",
    "    \"\"\"\n",
    "    n = len(V)\n",
    "    if W is None:\n",
    "        W = np.eye(n)\n",
    "    # Avoid singularities:\n",
    "    WPsi = W @ Psi\n",
    "    num = float(Psi @ WPsi)  # Psi' W Psi\n",
    "    if num <= 1e-30:\n",
    "        return math.nan, math.inf\n",
    "    R = float(Psi @ (W @ V)) / num\n",
    "    resid = V - R * Psi\n",
    "    obj = float(resid @ (W @ resid))\n",
    "    return R, obj\n",
    "\n",
    "\n",
    "def estimate_H_and_R_by_GMM(\n",
    "    V: np.ndarray,\n",
    "    windows_and_lags: List[Dict[str, int]],\n",
    "    H_min: float = H_MIN,\n",
    "    H_max: float = H_MAX,\n",
    "    mesh: float = H_MESH,\n",
    "    W: Optional[np.ndarray] = None,\n",
    ") -> GMMResult:\n",
    "    H_grid = np.arange(H_min, H_max + 1e-12, mesh, dtype=float)\n",
    "    objs = np.empty_like(H_grid)\n",
    "    Rs = np.empty_like(H_grid)\n",
    "\n",
    "    for i, H in enumerate(H_grid):\n",
    "        Psi = build_Psi_vector(H, windows_and_lags)\n",
    "        R, obj = gmm_objective_with_closed_form_R(V, Psi, W)\n",
    "        objs[i] = obj\n",
    "        Rs[i] = R\n",
    "\n",
    "    idx = int(np.nanargmin(objs))\n",
    "    H_hat = float(H_grid[idx])\n",
    "    R_hat = float(Rs[idx])\n",
    "    obj_star = float(objs[idx])\n",
    "    return GMMResult(H=H_hat, R=R_hat, obj=obj_star, H_grid=H_grid, obj_grid=objs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Heston calibration & MC (optional)\n",
    "# =========================\n",
    "@dataclass\n",
    "class HestonParams:\n",
    "    kappa: float\n",
    "    theta: float\n",
    "    xi: float\n",
    "    rho: float\n",
    "    v0: float\n",
    "    s0: float\n",
    "\n",
    "\n",
    "def calibrate_heston_from_returns(all_rets: List[np.ndarray], dt_years: float, s0: float) -> HestonParams:\n",
    "    \"\"\"Simple moment-matching; same spirit as user's version.\"\"\"\n",
    "    r = np.concatenate(all_rets)\n",
    "    v = (r ** 2) / max(dt_years, 1e-16)\n",
    "\n",
    "    v_t, v_tp = v[:-1], v[1:]\n",
    "    X = np.vstack([np.ones_like(v_t), v_t]).T\n",
    "    beta, *_ = np.linalg.lstsq(X, v_tp, rcond=None)\n",
    "    c, phi = float(beta[0]), float(beta[1])\n",
    "    phi = min(max(phi, -0.99), 0.999999)\n",
    "\n",
    "    kappa = max((1.0 - phi) / dt_years, 1e-8)\n",
    "    theta = max(c / (1.0 - phi), 1e-12)\n",
    "\n",
    "    e = v_tp - (c + phi * v_t)\n",
    "    denom = np.sum(v_t * dt_years)\n",
    "    xi2 = np.sum(e ** 2) / max(denom, 1e-16)\n",
    "    xi = float(np.sqrt(max(xi2, 1e-16)))\n",
    "\n",
    "    vt_dt = np.maximum(v_t * dt_years, 1e-16)\n",
    "    eta = (r[1:] + 0.5 * v_t * dt_years) / np.sqrt(vt_dt)\n",
    "    zeta = e / (xi * np.sqrt(vt_dt))\n",
    "    rho = float(np.corrcoef(eta, zeta)[0, 1])\n",
    "    rho = float(np.clip(rho, -0.999, 0.999))\n",
    "\n",
    "    return HestonParams(kappa=kappa, theta=theta, xi=xi, rho=rho, v0=float(theta), s0=float(s0))\n",
    "\n",
    "\n",
    "def heston_qe_step(s, v, dt, p: HestonParams, z1, z2):\n",
    "    \"\"\"Quadratic-Exponential step for variance; log-Euler for price.\"\"\"\n",
    "    z_v = z1\n",
    "    z_s = p.rho * z1 + np.sqrt(max(1.0 - p.rho ** 2, 0.0)) * z2\n",
    "\n",
    "    m = p.theta + (v - p.theta) * np.exp(-p.kappa * dt)\n",
    "    s2 = (v * p.xi ** 2 * np.exp(-p.kappa * dt) / p.kappa) * (1 - np.exp(-p.kappa * dt)) \\\n",
    "         + (p.theta * p.xi ** 2 / (2 * p.kappa)) * (1 - np.exp(-p.kappa * dt)) ** 2\n",
    "    psi = s2 / (m ** 2 + 1e-16)\n",
    "\n",
    "    if psi <= 1.5:\n",
    "        b2 = 2 / psi - 1 + np.sqrt(max(2 / psi, 0.0)) * np.sqrt(max(2 / psi - 1, 0.0))\n",
    "        a = m / (1 + b2)\n",
    "        v_next = a * (np.sqrt(max(b2, 0.0)) * z_v + 1) ** 2\n",
    "    else:\n",
    "        p_qe = (psi - 1) / (psi + 1)\n",
    "        beta = (1 - p_qe) / max(m, 1e-16)\n",
    "        U = 0.5 * (1 + math.erf(z_v / np.sqrt(2)))\n",
    "        v_next = 0.0 if U <= p_qe else -np.log((1 - p_qe) / (1 - U + 1e-16)) / max(beta, 1e-16)\n",
    "\n",
    "    v_bar = max(0.5 * (v + v_next), 0.0)\n",
    "    s_next = s * np.exp(-0.5 * v_bar * dt + np.sqrt(max(v_bar, 0.0) * dt) * z_s)\n",
    "    return s_next, max(v_next, 0.0)\n",
    "\n",
    "\n",
    "def simulate_heston_path_daily_independent(\n",
    "    p: HestonParams,\n",
    "    steps_fine: int,\n",
    "    sub_sample: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate ONE day independently:\n",
    "    - Start from unconditional mean for v0 (theta) and provided s0.\n",
    "    - Simulate on a fine grid (steps_fine), then subsample every 'sub_sample' steps.\n",
    "    Returns subsampled price and variance paths.\n",
    "    \"\"\"\n",
    "    s = p.s0\n",
    "    v = p.theta\n",
    "    Z = rng.standard_normal(size=(2, steps_fine))\n",
    "    s_path = np.empty(steps_fine + 1, dtype=float)\n",
    "    v_path = np.empty(steps_fine + 1, dtype=float)\n",
    "    s_path[0], v_path[0] = s, v\n",
    "    dt = 1.0 / (TRADING_DAYS_PER_YEAR * (6.5 * 3600.0 / FINE_DT_SECONDS))\n",
    "\n",
    "    for i in range(steps_fine):\n",
    "        s, v = heston_qe_step(s, v, dt, p, Z[0, i], Z[1, i])\n",
    "        s_path[i + 1], v_path[i + 1] = s, v\n",
    "\n",
    "    # Subsample\n",
    "    idx = np.arange(0, steps_fine + 1, sub_sample, dtype=int)\n",
    "    return s_path[idx], v_path[idx]\n",
    "\n",
    "\n",
    "def mc_generate_returns(\n",
    "    params: HestonParams,\n",
    "    n_days: int,\n",
    "    dt_sec_target: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate daily return arrays at target dt by:\n",
    "    - fine grid of FINE_DT_SECONDS\n",
    "    - subsample ratio = dt_sec_target / FINE_DT_SECONDS\n",
    "    - independent days reset to unconditional mean\n",
    "    \"\"\"\n",
    "    steps_fine_per_day = int((6.5 * 3600) // FINE_DT_SECONDS)\n",
    "    sub = int(round(dt_sec_target / FINE_DT_SECONDS))\n",
    "    if sub < 1:\n",
    "        sub = 1\n",
    "\n",
    "    out = []\n",
    "    p_today = HestonParams(**params.__dict__)\n",
    "    for _ in range(n_days):\n",
    "        s_path, _ = simulate_heston_path_daily_independent(p_today, steps_fine_per_day, sub, rng)\n",
    "        r = np.diff(np.log(s_path))\n",
    "        out.append(r)\n",
    "        # reset s0 to previous close (optional); v0 always resets to theta by design\n",
    "        p_today.s0 = float(s_path[-1])\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Driver\n",
    "# =========================\n",
    "def main():\n",
    "    # 0) Load & align\n",
    "    aligned, dt_sec = choose_dt_and_align(Path(CSV_PATH), verbose=True)\n",
    "    day_rets = to_daily_returns(aligned)\n",
    "    days_used = len(day_rets)\n",
    "    steps_per_day = len(day_rets[0])\n",
    "\n",
    "    print(f\"\\n[info] Days used: {days_used} | Bars/day: {steps_per_day} | dt={dt_sec}s\")\n",
    "\n",
    "    # 1) Build empirical moments V\n",
    "    V_emp, ac_emp = build_moment_vector_V(\n",
    "        day_rets,\n",
    "        windows_and_lags=PARAMS_TARGET,\n",
    "        trunc_c=TRUNCATION_C,\n",
    "        use_pattern=USE_PATTERN,\n",
    "    )\n",
    "    print(f\"[moments] V length = {len(V_emp)}\")\n",
    "\n",
    "    # 2) Estimate H & R by GMM\n",
    "    res_emp = estimate_H_and_R_by_GMM(V_emp, PARAMS_TARGET, H_MIN, H_MAX, H_MESH, W=None)\n",
    "    print(\"\\n========== EMPIRICAL (GMM) ==========\")\n",
    "    print(f\"H_hat = {res_emp.H:.3f} | R_hat = {res_emp.R:.4g} | obj* = {res_emp.obj:.4g}\")\n",
    "    # Simple boundary sentry:\n",
    "    if abs(res_emp.H - H_MIN) < 1e-6 or abs(res_emp.H - H_MAX) < 1e-6:\n",
    "        print(\"⚠️  H is at a boundary. Re-check V/Psi stacking, pattern, truncation and windows.\")\n",
    "\n",
    "    # 3) Calibrate Heston from observed returns (for optional MC)\n",
    "    first_day = aligned[\"date\"].min()\n",
    "    s0 = float(aligned.loc[aligned[\"date\"] == first_day, \"Price\"].iloc[0])\n",
    "    dt_years = dt_sec / (TRADING_DAYS_PER_YEAR * 6.5 * 3600.0)\n",
    "    heston_params = calibrate_heston_from_returns(day_rets, dt_years, s0)\n",
    "\n",
    "    print(\"\\n===== HESTON CALIBRATION (from data) =====\")\n",
    "    for k, v in heston_params.__dict__.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k:6s}: {v:.6g}\")\n",
    "        else:\n",
    "            print(f\"  {k:6s}: {v}\")\n",
    "\n",
    "    # 4) Optional Monte Carlo study\n",
    "    if DO_MC:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        H_list = []\n",
    "        for _ in range(N_MC):\n",
    "            mc_rets = mc_generate_returns(heston_params, n_days=days_used, dt_sec_target=dt_sec, rng=rng)\n",
    "            V_mc, _ = build_moment_vector_V(\n",
    "                mc_rets,\n",
    "                windows_and_lags=PARAMS_TARGET,\n",
    "                trunc_c=TRUNCATION_C,\n",
    "                use_pattern=USE_PATTERN,\n",
    "            )\n",
    "            res_mc = estimate_H_and_R_by_GMM(V_mc, PARAMS_TARGET, H_MIN, H_MAX, H_MESH, W=None)\n",
    "            H_list.append(res_mc.H)\n",
    "\n",
    "        H_arr = np.array(H_list, float)\n",
    "        print(\"\\n========== MONTE CARLO ==========\")\n",
    "        print(f\"Runs: {N_MC} | H_mean={np.nanmean(H_arr):.3f} | H_sd={np.nanstd(H_arr, ddof=1):.3f} \"\n",
    "              f\"| [p5, p95]=[{np.nanpercentile(H_arr,5):.3f}, {np.nanpercentile(H_arr,95):.3f}]\")\n",
    "\n",
    "    # 5) Quick sanity prints for diagnostics\n",
    "    print(\"\\n[diagnostics] Per-window γℓ previews (empirical increments):\")\n",
    "    for k, gam in ac_emp.items():\n",
    "        preview = \", \".join([f\"γ{ell}={gam[ell]:.4g}\" for ell in range(0, min(5, len(gam)))])\n",
    "        print(f\"  {k:>6s}: {preview}\")\n",
    "\n",
    "    # Check sign/shape vs Psi at a few H values\n",
    "    for Hprobe in (0.2, 0.3, 0.4):\n",
    "        Psi_probe = build_Psi_vector(Hprobe, PARAMS_TARGET)\n",
    "        corr = np.corrcoef(V_emp, Psi_probe)[0, 1]\n",
    "        print(f\"[diagnostics] corr(V, Psi({Hprobe:.2f})) = {corr:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0632402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
