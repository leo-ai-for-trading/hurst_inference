{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370082f6",
   "metadata": {},
   "source": [
    "# ROBUST INFERENCE OF VOLATILITY ROUGHNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(r\"daily_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe83f7",
   "metadata": {},
   "source": [
    "## First Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891aa049",
   "metadata": {},
   "source": [
    "\n",
    "  $$\n",
    "  \\hat c^{n,k_n}*i ;=; \\frac{1}{k_n\\varepsilon_n}\\sum*{j=i}^{i+k_n-1} (\\Delta y_j)^2 \\mathbf{1}_{{|\\Delta y_j|\\le v_n}},\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b58abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "##############################\n",
    "### REMOVING FOLLOWING DATES #\n",
    "##############################\n",
    "\n",
    "FOMC_announcement = [\n",
    "    \"2012-01-25\",\"2012-03-13\",\"2012-04-25\",\"2012-06-20\",\"2012-08-01\",\"2012-09-13\",\"2012-10-24\",\"2012-12-12\",\n",
    "    \"2013-01-30\",\"2013-03-20\",\"2013-05-01\",\"2013-06-19\",\"2013-07-31\",\"2013-09-18\",\"2013-10-30\",\"2013-12-18\",\n",
    "    \"2014-01-29\",\"2014-03-19\",\"2014-04-30\",\"2014-06-18\",\"2014-07-30\",\"2014-09-17\",\"2014-10-29\",\"2014-12-17\",\n",
    "    \"2015-01-28\",\"2015-03-18\",\"2015-04-29\",\"2015-06-17\",\"2015-07-29\",\"2015-09-17\",\"2015-10-28\",\"2015-12-16\",\n",
    "    \"2016-01-27\",\"2016-03-16\",\"2016-04-27\",\"2016-06-15\",\"2016-07-27\",\"2016-09-21\",\"2016-11-02\",\"2016-12-14\",\n",
    "    \"2017-02-01\",\"2017-03-15\",\"2017-05-03\",\"2017-06-14\",\"2017-07-26\",\"2017-09-20\",\"2017-11-01\",\"2017-12-13\",\n",
    "    \"2018-01-31\",\"2018-03-21\",\"2018-05-02\",\"2018-06-13\",\"2018-08-01\",\"2018-09-26\",\"2018-11-08\",\"2018-12-19\",\n",
    "    \"2019-01-30\",\"2019-03-20\",\"2019-05-01\",\"2019-06-19\",\"2019-07-31\",\"2019-09-18\",\"2019-10-30\",\"2019-12-11\",\n",
    "    \"2020-01-29\",\"2020-04-29\",\"2020-06-10\",\"2020-07-29\",\"2020-09-16\",\"2020-11-05\",\"2020-12-16\",\n",
    "    \"2021-01-27\",\"2021-03-17\",\"2021-04-28\",\"2021-06-16\",\"2021-07-28\",\"2021-09-22\",\"2021-11-03\",\"2021-12-15\",\n",
    "    \"2022-01-26\",\"2022-03-16\",\"2022-05-04\",\"2022-06-15\",\"2022-07-27\",\"2022-09-21\",\"2022-11-02\",\"2022-12-14\",\n",
    "]\n",
    "trading_halt = [\n",
    "    '2013-07-03','2013-11-29','2013-12-24',\n",
    "    '2014-07-03','2014-10-30','2014-11-28','2014-12-24',\n",
    "    '2015-11-27','2015-12-24',\n",
    "    '2016-11-25',\n",
    "    '2017-07-03','2017-11-24',\n",
    "    '2018-07-03','2018-11-23','2018-12-24',\n",
    "    \"2019-07-03\",\"2019-08-12\",\"2019-11-29\",\"2019-12-24\",\n",
    "    \"2020-03-09\",\"2020-03-12\",\"2020-03-16\",\"2020-03-18\",\"2020-11-27\",\"2020-12-24\",\n",
    "    \"2021-05-05\",\"2022-11-26\",\"2022-11-25\"\n",
    "]\n",
    "EXCLUDE_DATES = {date.fromisoformat(s) for s in FOMC_announcement} | {date.fromisoformat(s) for s in trading_halt}\n",
    "\n",
    "###################################################\n",
    "### READING DATA USING POLARS #####################\n",
    "###################################################\n",
    "\n",
    "def load_daily_dir_to_polars(\n",
    "    dir_path: str | Path = \"daily_csv\",\n",
    "    *,\n",
    "    dt_col: str = \"DT\",\n",
    "    px_col: str = \"Price\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all CSVs in dir_path with columns [DT, Price], returns a single\n",
    "    Polars DataFrame with standardized columns ['timestamp','price'].\n",
    "    Skips empty/iCloud placeholder files (size == 0).\n",
    "    \"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    files = sorted(p for p in dir_path.glob(\"*.csv\") if p.is_file() and p.stat().st_size > 0)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No non-empty CSV files in {dir_path.resolve()}\")\n",
    "\n",
    "    \n",
    "    lfs = []\n",
    "    for p in files:\n",
    "        lf = (\n",
    "            pl.scan_csv(\n",
    "                p,\n",
    "                has_header=True,\n",
    "                infer_schema_length=0,  \n",
    "                ignore_errors=True\n",
    "            )\n",
    "            .select([pl.col(dt_col).alias(\"DT\"), pl.col(px_col).cast(pl.Float64).alias(\"Price\")])\n",
    "            .with_columns([\n",
    "                # Parse DT to Datetime (auto-detects common formats; tolerant)\n",
    "                pl.col(\"DT\").str.strptime(pl.Datetime, strict=False).alias(\"timestamp\"),\n",
    "                pl.col(\"Price\").alias(\"price\"),\n",
    "            ])\n",
    "            .select([\"timestamp\",\"price\"])\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    df = (\n",
    "        pl.concat(lfs, how=\"vertical_relaxed\")\n",
    "        .drop_nulls([\"timestamp\",\"price\"])\n",
    "        .unique(subset=[\"timestamp\"], keep=\"last\")\n",
    "        .sort(\"timestamp\")\n",
    "        .collect(streaming=True)   \n",
    "        .rechunk()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "##########################################################################\n",
    "############ JUMP TRUNCATION #############################################\n",
    "##########################################################################\n",
    "\n",
    "def infer_eps_seconds(df: pl.DataFrame, ts_col: str = \"timestamp\") -> float:\n",
    "    \"\"\"Infer ε_n (seconds) from median spacing within each day.\"\"\"\n",
    "    return float(\n",
    "        df\n",
    "        .with_columns(pl.col(ts_col).dt.date().alias(\"date\"))\n",
    "        .group_by(\"date\")\n",
    "        .agg(pl.col(ts_col).diff().drop_nulls().alias(\"dts\"))\n",
    "        .explode(\"dts\")\n",
    "        .select(pl.col(\"dts\").dt.total_seconds().alias(\"sec\"))\n",
    "        .select(pl.median(\"sec\").alias(\"eps\"))\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "def filter_jumps_paper_threshold(\n",
    "    df: pl.DataFrame,\n",
    "    *,\n",
    "    ts_col: str = \"timestamp\",\n",
    "    px_col: str = \"price\",\n",
    "    gamma: float = 0.49,\n",
    "    kappa: float = 8.0,\n",
    "    eps_seconds: float | None = None,\n",
    "    drop_excluded_days: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply |Δy| <= v_n with v_n = kappa * ε_n^gamma (microstructure ignored).\n",
    "    Returns (out_df, clean_df, v_n, eps_seconds).\n",
    "    out_df keeps rows and sets jump returns to NULL in 'ret_nojump';\n",
    "    clean_df drops jump increments entirely.\n",
    "    \"\"\"\n",
    "    df0 = df\n",
    "    if drop_excluded_days:\n",
    "        df0 = (\n",
    "            df0\n",
    "            .with_columns(pl.col(ts_col).dt.date().alias(\"date\"))\n",
    "            .filter(~pl.col(\"date\").is_in(list(EXCLUDE_DATES)))\n",
    "            .drop(\"date\")\n",
    "        )\n",
    "\n",
    "    if eps_seconds is None:\n",
    "        eps_seconds = infer_eps_seconds(df0, ts_col)\n",
    "\n",
    "    df1 = (\n",
    "        df0\n",
    "        .with_columns([\n",
    "            pl.col(ts_col).dt.date().alias(\"date\"),\n",
    "            pl.col(px_col).log().alias(\"logp\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"date\") == pl.col(\"date\").shift(1))\n",
    "              .then(pl.col(\"logp\") - pl.col(\"logp\").shift(1))\n",
    "              .otherwise(None)\n",
    "              .alias(\"ret\")\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    v_n = kappa * (eps_seconds ** gamma)\n",
    "\n",
    "    out = (\n",
    "        df1\n",
    "        .with_columns([\n",
    "            pl.col(\"ret\").abs().alias(\"abs_ret\"),\n",
    "            pl.lit(v_n).alias(\"v_n\"),\n",
    "            (pl.col(\"ret\").abs() > v_n).alias(\"is_jump\"),\n",
    "            pl.when(pl.col(\"ret\").abs() > v_n).then(None).otherwise(pl.col(\"ret\")).alias(\"ret_nojump\")\n",
    "        ])\n",
    "    )\n",
    "    clean = out.filter(~pl.col(\"is_jump\").fill_null(False))\n",
    "    return out, clean, v_n, eps_seconds\n",
    "\n",
    "############################################\n",
    "############ RUN ###########################\n",
    "############################################\n",
    "\n",
    "df = load_daily_dir_to_polars(\"daily_csv\")\n",
    "print(df.head())\n",
    "print(df.schema)\n",
    "\n",
    "\n",
    "out_df, clean_df, v_n, eps = filter_jumps_paper_threshold(\n",
    "    df,\n",
    "    ts_col=\"timestamp\",\n",
    "    px_col=\"price\",\n",
    "    gamma=0.49,      \n",
    "    kappa=8.0,       \n",
    "    eps_seconds=None,\n",
    "    drop_excluded_days=True\n",
    ")\n",
    "\n",
    "print(f\"ε_n (seconds) = {eps:.6g} | v_n = {v_n:.6g}\")\n",
    "\n",
    "\n",
    "summary = (\n",
    "    out_df\n",
    "    .select([\n",
    "        pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"is_jump\")\n",
    "    ])\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_obs\"),\n",
    "        pl.col(\"is_jump\").sum().fill_null(0).alias(\"n_jumps\")\n",
    "    ])\n",
    "    .sort(\"date\")\n",
    "    .tail(15)\n",
    ")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c63cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "############################## PLOT ###########################\n",
    "###############################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date as _pydate\n",
    "\n",
    "plot_df = (\n",
    "    out_df\n",
    "    .with_columns([\n",
    "        pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"ret_nojump\").fill_null(0.0).alias(\"ret_nj0\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"ret_nj0\").cum_sum().over(\"date\").alias(\"cs_ret_nj0\"),\n",
    "        pl.first(\"logp\").over(\"date\").alias(\"logp0\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"logp0\") + pl.col(\"cs_ret_nj0\")).exp().alias(\"price_nojump\")\n",
    "    ])\n",
    "    .select([\"timestamp\", \"date\", \"price\", \"price_nojump\", \"ret\", \"ret_nojump\", \"is_jump\", \"v_n\"])\n",
    "    .sort(\"timestamp\")\n",
    ")\n",
    "\n",
    "\n",
    "_last_day = plot_df.select(pl.max(\"date\")).item()\n",
    "DAY = str(_last_day)  \n",
    "_day = _pydate.fromisoformat(DAY)\n",
    "\n",
    "day_view = (\n",
    "    plot_df\n",
    "    .filter(pl.col(\"date\") == pl.lit(_day))\n",
    "    .select([\"timestamp\",\"price\",\"price_nojump\",\"ret\",\"ret_nojump\",\"is_jump\",\"v_n\"])\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.plot(day_view[\"timestamp\"], day_view[\"price\"], label=\"raw price\")\n",
    "plt.plot(day_view[\"timestamp\"], day_view[\"price_nojump\"], label=\"de-jumped price\")\n",
    "plt.title(f\"Price vs De-jumped Price — {DAY}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.plot(day_view[\"timestamp\"], day_view[\"ret\"], label=\"raw returns\")\n",
    "plt.plot(day_view[\"timestamp\"], day_view[\"ret_nojump\"], label=\"returns (jumps removed)\")\n",
    "if len(day_view) > 0:\n",
    "    _vn = float(day_view[\"v_n\"].iloc[0])\n",
    "    plt.axhline(y=_vn, linestyle=\"--\")\n",
    "    plt.axhline(y=-_vn, linestyle=\"--\")\n",
    "\n",
    "if \"is_jump\" in day_view.columns:\n",
    "    jump_idx = day_view[\"is_jump\"] == True\n",
    "    if jump_idx.any():\n",
    "        plt.scatter(day_view.loc[jump_idx, \"timestamp\"],\n",
    "                    day_view.loc[jump_idx, \"ret\"],\n",
    "                    marker=\"x\")\n",
    "plt.title(f\"Intraday Returns with ±vₙ — {DAY}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################## SANITY CHECKS ###########################\n",
    "############################################################\n",
    "\n",
    "TOL = 1e-12  \n",
    "\n",
    "\n",
    "base = out_df.filter(pl.col(\"ret\").is_not_null())\n",
    "\n",
    "\n",
    "false_neg = base.filter(~pl.col(\"is_jump\") & (pl.col(\"abs_ret\") > pl.col(\"v_n\") + TOL)).height\n",
    "false_pos = base.filter(pl.col(\"is_jump\") & (pl.col(\"abs_ret\") <= pl.col(\"v_n\") - TOL)).height\n",
    "\n",
    "\n",
    "mismatch_keep = base.filter(\n",
    "    (~pl.col(\"is_jump\")) &\n",
    "    ((pl.col(\"ret_nojump\") - pl.col(\"ret\")).abs() > TOL)\n",
    ").height\n",
    "\n",
    "mismatch_drop = base.filter(\n",
    "    (pl.col(\"is_jump\")) & pl.col(\"ret_nojump\").is_not_null()\n",
    ").height\n",
    "\n",
    "\n",
    "max_kept_minus_vn = float(\n",
    "    base.filter(~pl.col(\"is_jump\"))\n",
    "        .select((pl.max(\"abs_ret\") - pl.first(\"v_n\")).alias(\"diff\"))\n",
    "        .item()\n",
    ")\n",
    "\n",
    "\n",
    "present_excluded_days = (\n",
    "    out_df\n",
    "    .select(pl.col(\"timestamp\").dt.date().alias(\"date\"))\n",
    "    .unique()\n",
    "    .filter(pl.col(\"date\").is_in(list(EXCLUDE_DATES)))\n",
    "    .height\n",
    ")\n",
    "\n",
    "print(\"=== Sanity: logical constraints ===\")\n",
    "print(f\"False negatives (kept but |ret|>v_n): {false_neg}\")\n",
    "print(f\"False positives (flagged jump but |ret|<=v_n): {false_pos}\")\n",
    "print(f\"Mismatch kept (ret_nojump ≠ ret when not a jump): {mismatch_keep}\")\n",
    "print(f\"Mismatch dropped (ret_nojump not NULL on a jump): {mismatch_drop}\")\n",
    "print(f\"Max(|ret| among kept) - v_n: {max_kept_minus_vn:.3e}\")\n",
    "print(f\"Excluded days still present: {present_excluded_days}\")\n",
    "\n",
    "assert false_neg == 0, \"Some big returns were NOT flagged as jumps.\"\n",
    "assert false_pos == 0, \"Some flagged jumps are not actually > v_n.\"\n",
    "assert mismatch_keep == 0, \"ret_nojump must equal ret on non-jump rows.\"\n",
    "assert mismatch_drop == 0, \"ret_nojump must be NULL on jump rows.\"\n",
    "assert max_kept_minus_vn <= TOL, \"A kept return exceeds v_n.\"\n",
    "assert present_excluded_days == 0, \"Some excluded calendar dates remain.\"\n",
    "\n",
    "dropped_share = float(\n",
    "    base.select(pl.col(\"is_jump\").fill_null(False).mean().alias(\"share_dropped\")).item()\n",
    ")\n",
    "print(\"\\n=== Diagnostics ===\")\n",
    "print(f\"Share of increments dropped as jumps: {dropped_share:.4%}\")\n",
    "\n",
    "q = (\n",
    "    base.select([\n",
    "        pl.col(\"ret\").abs().quantile(0.999).alias(\"q999_raw\"),\n",
    "        pl.col(\"ret_nojump\").abs().quantile(0.999).alias(\"q999_trunc\"),\n",
    "        pl.col(\"abs_ret\").max().alias(\"max_abs_raw\"),\n",
    "    ])\n",
    "    .to_dicts()[0]\n",
    ")\n",
    "print(f\"abs(ret) 99.9% quantile: raw={q['q999_raw']:.3e}  |  truncated={q['q999_trunc']:.3e}\")\n",
    "print(f\"abs(ret) max (raw): {q['max_abs_raw']:.3e}   |  v_n={v_n:.3e}\")\n",
    "\n",
    "rv_daily = (\n",
    "    out_df\n",
    "    .select([\n",
    "        pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "        (pl.col(\"ret\")**2).alias(\"rv_raw\"),\n",
    "        (pl.col(\"ret_nojump\")**2).alias(\"rv_trunc\")\n",
    "    ])\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.sum(\"rv_raw\").alias(\"RV_raw\"),\n",
    "        pl.sum(\"rv_trunc\").alias(\"RV_trunc\"),\n",
    "    ])\n",
    "    .with_columns((pl.col(\"RV_trunc\") / pl.col(\"RV_raw\")).alias(\"ratio\"))\n",
    "    .sort(\"date\")\n",
    ")\n",
    "\n",
    "rv_stats = rv_daily.select([\n",
    "    pl.col(\"ratio\").mean().alias(\"mean_ratio\"),\n",
    "    pl.col(\"ratio\").quantile(0.1).alias(\"p10\"),\n",
    "    pl.col(\"ratio\").quantile(0.5).alias(\"p50\"),\n",
    "    pl.col(\"ratio\").quantile(0.9).alias(\"p90\"),\n",
    "]).to_dicts()[0]\n",
    "\n",
    "print(\"\\n=== Realized-variance check (RV_trunc / RV_raw) ===\")\n",
    "print(f\"mean={rv_stats['mean_ratio']:.3f} | p10={rv_stats['p10']:.3f} | \"\n",
    "      f\"p50={rv_stats['p50']:.3f} | p90={rv_stats['p90']:.3f}\")\n",
    "\n",
    "jump_summary = (\n",
    "    out_df\n",
    "    .select([\n",
    "        pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"is_jump\")\n",
    "    ])\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_obs\"),\n",
    "        pl.col(\"is_jump\").sum().fill_null(0).cast(pl.Int64).alias(\"n_jumps\")\n",
    "    ])\n",
    "    .with_columns((pl.col(\"n_jumps\") / pl.col(\"n_obs\")).alias(\"jump_rate\"))\n",
    "    .sort(\"jump_rate\", descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "print(\"\\nTop-10 days by jump rate:\")\n",
    "print(jump_summary)\n",
    "\n",
    "impact_summary = (\n",
    "    rv_daily\n",
    "    .with_columns((1 - pl.col(\"ratio\")).alias(\"rv_reduction\"))\n",
    "    .sort(\"rv_reduction\", descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "print(\"\\nTop-10 days by RV reduction:\")\n",
    "print(impact_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bbaf59",
   "metadata": {},
   "source": [
    "## Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import date as Date\n",
    "\n",
    "######################################################################################\n",
    "################## REMOVE DATES ######################################################\n",
    "######################################################################################\n",
    "FOMC_announcement = [\n",
    "    \"2012-01-25\",\"2012-03-13\",\"2012-04-25\",\"2012-06-20\",\"2012-08-01\",\"2012-09-13\",\"2012-10-24\",\"2012-12-12\",\n",
    "    \"2013-01-30\",\"2013-03-20\",\"2013-05-01\",\"2013-06-19\",\"2013-07-31\",\"2013-09-18\",\"2013-10-30\",\"2013-12-18\",\n",
    "    \"2014-01-29\",\"2014-03-19\",\"2014-04-30\",\"2014-06-18\",\"2014-07-30\",\"2014-09-17\",\"2014-10-29\",\"2014-12-17\",\n",
    "    \"2015-01-28\",\"2015-03-18\",\"2015-04-29\",\"2015-06-17\",\"2015-07-29\",\"2015-09-17\",\"2015-10-28\",\"2015-12-16\",\n",
    "    \"2016-01-27\",\"2016-03-16\",\"2016-04-27\",\"2016-06-15\",\"2016-07-27\",\"2016-09-21\",\"2016-11-02\",\"2016-12-14\",\n",
    "    \"2017-02-01\",\"2017-03-15\",\"2017-05-03\",\"2017-06-14\",\"2017-07-26\",\"2017-09-20\",\"2017-11-01\",\"2017-12-13\",\n",
    "    \"2018-01-31\",\"2018-03-21\",\"2018-05-02\",\"2018-06-13\",\"2018-08-01\",\"2018-09-26\",\"2018-11-08\",\"2018-12-19\",\n",
    "    \"2019-01-30\",\"2019-03-20\",\"2019-05-01\",\"2019-06-19\",\"2019-07-31\",\"2019-09-18\",\"2019-10-30\",\"2019-12-11\",\n",
    "    \"2020-01-29\",\"2020-04-29\",\"2020-06-10\",\"2020-07-29\",\"2020-09-16\",\"2020-11-05\",\"2020-12-16\",\n",
    "    \"2021-01-27\",\"2021-03-17\",\"2021-04-28\",\"2021-06-16\",\"2021-07-28\",\"2021-09-22\",\"2021-11-03\",\"2021-12-15\",\n",
    "    \"2022-01-26\",\"2022-03-16\",\"2022-05-04\",\"2022-06-15\",\"2022-07-27\",\"2022-09-21\",\"2022-11-02\",\"2022-12-14\",\n",
    "]\n",
    "trading_halt = [\n",
    "    '2013-07-03','2013-11-29','2013-12-24',\n",
    "    '2014-07-03','2014-10-30','2014-11-28','2014-12-24',\n",
    "    '2015-11-27','2015-12-24',\n",
    "    '2016-11-25',\n",
    "    '2017-07-03','2017-11-24',\n",
    "    '2018-07-03','2018-11-23','2018-12-24',\n",
    "    \"2019-07-03\",\"2019-08-12\",\"2019-11-29\",\"2019-12-24\",\n",
    "    \"2020-03-09\",\"2020-03-12\",\"2020-03-16\",\"2020-03-18\",\"2020-11-27\",\"2020-12-24\",\n",
    "    \"2021-05-05\",\"2022-11-26\",\"2022-11-25\"\n",
    "]\n",
    "EXCLUDE_DATES = {Date.fromisoformat(s) for s in FOMC_announcement} | {Date.fromisoformat(s) for s in trading_halt}\n",
    "\n",
    "####################################################################################\n",
    "################## LOAD FILES ######################################################\n",
    "####################################################################################\n",
    "def load_daily_dir_to_polars(\n",
    "    dir_path: str | Path = \"daily_csv\",\n",
    "    *,\n",
    "    dt_col: str = \"DT\",\n",
    "    px_col: str = \"Price\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all CSVs in dir_path with columns [DT, Price] -> ['timestamp','price'].\n",
    "    Skips empty/iCloud placeholder files (size == 0). Uses scan_csv for speed.\n",
    "    \"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    files = sorted(p for p in dir_path.glob(\"*.csv\") if p.is_file() and p.stat().st_size > 0)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No non-empty CSV files in {dir_path.resolve()}\")\n",
    "\n",
    "    lfs = []\n",
    "    for p in files:\n",
    "        lf = (\n",
    "            pl.scan_csv(p, has_header=True, infer_schema_length=0, ignore_errors=True)\n",
    "            .select([\n",
    "                pl.col(dt_col).alias(\"DT\"),\n",
    "                pl.col(px_col).cast(pl.Float64).alias(\"Price\"),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.col(\"DT\").str.strptime(pl.Datetime, strict=False).alias(\"timestamp\"),\n",
    "                pl.col(\"Price\").alias(\"price\"),\n",
    "            ])\n",
    "            .select([\"timestamp\",\"price\"])\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    df = (\n",
    "        pl.concat(lfs, how=\"vertical_relaxed\")\n",
    "        .drop_nulls([\"timestamp\",\"price\"])\n",
    "        .unique(subset=[\"timestamp\"], keep=\"last\")\n",
    "        .sort(\"timestamp\")\n",
    "        .collect(streaming=True)\n",
    "        .rechunk()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "#####################################################################################\n",
    "################## JUMP FILTER ######################################################\n",
    "#####################################################################################\n",
    "\n",
    "def infer_eps_seconds(df: pl.DataFrame, ts_col: str = \"timestamp\") -> float:\n",
    "    \"\"\"Infer ε_n (seconds) from median within-day spacing.\"\"\"\n",
    "    return float(\n",
    "        df.with_columns(pl.col(ts_col).dt.date().alias(\"date\"))\n",
    "          .group_by(\"date\")\n",
    "          .agg(pl.col(ts_col).diff().drop_nulls().alias(\"dts\"))\n",
    "          .explode(\"dts\")\n",
    "          .select(pl.col(\"dts\").dt.total_seconds().alias(\"sec\"))\n",
    "          .select(pl.median(\"sec\").alias(\"eps\"))\n",
    "          .item()\n",
    "    )\n",
    "\n",
    "def filter_jumps_paper_threshold(\n",
    "    df: pl.DataFrame,\n",
    "    *,\n",
    "    ts_col: str = \"timestamp\",\n",
    "    px_col: str = \"price\",\n",
    "    gamma: float = 0.49,\n",
    "    kappa: float = 8.0,\n",
    "    eps_seconds: float | None = None,\n",
    "    drop_excluded_days: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply |Δy| <= v_n with v_n = kappa * ε_n^gamma (ignore microstructure noise).\n",
    "    Returns (out_df, clean_df, v_n, eps_seconds).\n",
    "    - out_df keeps all rows and sets jump returns to NULL in 'ret_nojump'\n",
    "    - clean_df drops jump increments entirely\n",
    "    \"\"\"\n",
    "    df0 = df\n",
    "    if drop_excluded_days:\n",
    "        df0 = (\n",
    "            df0\n",
    "            .with_columns(pl.col(ts_col).dt.date().alias(\"date\"))\n",
    "            .filter(~pl.col(\"date\").is_in(list(EXCLUDE_DATES)))\n",
    "            .drop(\"date\")\n",
    "        )\n",
    "\n",
    "    if eps_seconds is None:\n",
    "        eps_seconds = infer_eps_seconds(df0, ts_col)\n",
    "\n",
    "    df1 = (\n",
    "        df0\n",
    "        .with_columns([\n",
    "            pl.col(ts_col).dt.date().alias(\"date\"),\n",
    "            pl.col(px_col).log().alias(\"logp\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"date\") == pl.col(\"date\").shift(1))\n",
    "              .then(pl.col(\"logp\") - pl.col(\"logp\").shift(1))\n",
    "              .otherwise(None)\n",
    "              .alias(\"ret\")\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    v_n = kappa * (eps_seconds ** gamma)\n",
    "\n",
    "    out = (\n",
    "        df1\n",
    "        .with_columns([\n",
    "            pl.col(\"ret\").abs().alias(\"abs_ret\"),\n",
    "            pl.lit(v_n).alias(\"v_n\"),\n",
    "            (pl.col(\"ret\").abs() > v_n).alias(\"is_jump\"),\n",
    "            pl.when(pl.col(\"ret\").abs() > v_n).then(None).otherwise(pl.col(\"ret\")).alias(\"ret_nojump\"),\n",
    "        ])\n",
    "    )\n",
    "    clean = out.filter(~pl.col(\"is_jump\").fill_null(False))\n",
    "    return out, clean, v_n, eps_seconds\n",
    "\n",
    "####################################################################\n",
    "################## SANITY CHECK ####################################\n",
    "####################################################################\n",
    "\n",
    "def sanity_checks(out_df: pl.DataFrame, v_n: float, eps: float):\n",
    "    TOL = 1e-12\n",
    "    base = out_df.filter(pl.col(\"ret\").is_not_null())\n",
    "\n",
    "    false_neg = base.filter(~pl.col(\"is_jump\") & (pl.col(\"abs_ret\") > pl.col(\"v_n\") + TOL)).height\n",
    "    false_pos = base.filter(pl.col(\"is_jump\") & (pl.col(\"abs_ret\") <= pl.col(\"v_n\") - TOL)).height\n",
    "\n",
    "    mismatch_keep = base.filter(\n",
    "        (~pl.col(\"is_jump\")) & ((pl.col(\"ret_nojump\") - pl.col(\"ret\")).abs() > TOL)\n",
    "    ).height\n",
    "    mismatch_drop = base.filter((pl.col(\"is_jump\")) & pl.col(\"ret_nojump\").is_not_null()).height\n",
    "\n",
    "    max_kept_minus_vn = float(\n",
    "        base.filter(~pl.col(\"is_jump\"))\n",
    "            .select((pl.col(\"abs_ret\").max() - pl.col(\"v_n\").first()).alias(\"diff\"))\n",
    "            .item()\n",
    "    )\n",
    "\n",
    "    present_excluded_days = (\n",
    "        out_df\n",
    "        .select(pl.col(\"timestamp\").dt.date().alias(\"date\"))\n",
    "        .unique()\n",
    "        .filter(pl.col(\"date\").is_in(list(EXCLUDE_DATES)))\n",
    "        .height\n",
    "    )\n",
    "\n",
    "    print(\"=== Sanity: logical constraints ===\")\n",
    "    print(f\"False negatives (kept but |ret|>v_n): {false_neg}\")\n",
    "    print(f\"False positives (flagged jump but |ret|<=v_n): {false_pos}\")\n",
    "    print(f\"Mismatch kept (ret_nojump ≠ ret when not a jump): {mismatch_keep}\")\n",
    "    print(f\"Mismatch dropped (ret_nojump not NULL on a jump): {mismatch_drop}\")\n",
    "    print(f\"Max(|ret| among kept) - v_n: {max_kept_minus_vn:.3e}\")\n",
    "    print(f\"Excluded days still present: {present_excluded_days}\")\n",
    "\n",
    "    assert false_neg == 0\n",
    "    assert false_pos == 0\n",
    "    assert mismatch_keep == 0\n",
    "    assert mismatch_drop == 0\n",
    "    assert max_kept_minus_vn <= TOL\n",
    "    assert present_excluded_days == 0\n",
    "\n",
    "    dropped_share = float(\n",
    "        base.select(pl.col(\"is_jump\").fill_null(False).mean().alias(\"share_dropped\")).item()\n",
    "    )\n",
    "    print(\"\\n=== Diagnostics ===\")\n",
    "    print(f\"Share of increments dropped as jumps: {dropped_share:.4%}\")\n",
    "\n",
    "    q = (\n",
    "        base.select([\n",
    "            pl.col(\"ret\").abs().quantile(0.999).alias(\"q999_raw\"),\n",
    "            pl.col(\"ret_nojump\").abs().quantile(0.999).alias(\"q999_trunc\"),\n",
    "            pl.col(\"abs_ret\").max().alias(\"max_abs_raw\"),\n",
    "        ])\n",
    "        .to_dicts()[0]\n",
    "    )\n",
    "    print(f\"abs(ret) 99.9% quantile: raw={q['q999_raw']:.3e}  |  truncated={q['q999_trunc']:.3e}\")\n",
    "    print(f\"abs(ret) max (raw): {q['max_abs_raw']:.3e}   |  v_n={v_n:.3e}\")\n",
    "\n",
    "    rv_daily = (\n",
    "        out_df\n",
    "        .select([\n",
    "            pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "            (pl.col(\"ret\")**2).alias(\"rv_raw\"),\n",
    "            (pl.col(\"ret_nojump\")**2).alias(\"rv_trunc\")\n",
    "        ])\n",
    "        .group_by(\"date\")\n",
    "        .agg([\n",
    "            pl.sum(\"rv_raw\").alias(\"RV_raw\"),\n",
    "            pl.sum(\"rv_trunc\").alias(\"RV_trunc\"),\n",
    "        ])\n",
    "        .with_columns((pl.col(\"RV_trunc\") / pl.col(\"RV_raw\")).alias(\"ratio\"))\n",
    "        .sort(\"date\")\n",
    "    )\n",
    "\n",
    "    rv_stats = rv_daily.select([\n",
    "        pl.col(\"ratio\").mean().alias(\"mean_ratio\"),\n",
    "        pl.col(\"ratio\").quantile(0.1).alias(\"p10\"),\n",
    "        pl.col(\"ratio\").quantile(0.5).alias(\"p50\"),\n",
    "        pl.col(\"ratio\").quantile(0.9).alias(\"p90\"),\n",
    "    ]).to_dicts()[0]\n",
    "\n",
    "    print(\"\\n=== Realized-variance check (RV_trunc / RV_raw) ===\")\n",
    "    print(f\"mean={rv_stats['mean_ratio']:.3f} | p10={rv_stats['p10']:.3f} | \"\n",
    "          f\"p50={rv_stats['p50']:.3f} | p90={rv_stats['p90']:.3f}\")\n",
    "\n",
    "    jump_summary = (\n",
    "        out_df\n",
    "        .select([\n",
    "            pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "            pl.col(\"is_jump\")\n",
    "        ])\n",
    "        .group_by(\"date\")\n",
    "        .agg([\n",
    "            pl.len().alias(\"n_obs\"),\n",
    "            pl.col(\"is_jump\").sum().fill_null(0).cast(pl.Int64).alias(\"n_jumps\")\n",
    "        ])\n",
    "        .with_columns((pl.col(\"n_jumps\") / pl.col(\"n_obs\")).alias(\"jump_rate\"))\n",
    "        .sort(\"jump_rate\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\nTop-10 days by jump rate:\")\n",
    "    print(jump_summary)\n",
    "\n",
    "    impact_summary = (\n",
    "        rv_daily\n",
    "        .with_columns((1 - pl.col(\"ratio\")).alias(\"rv_reduction\"))\n",
    "        .sort(\"rv_reduction\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\nTop-10 days by RV reduction:\")\n",
    "    print(impact_summary)\n",
    "\n",
    "###########################################################################################\n",
    "################## KAPPA CALIBRATION ######################################################\n",
    "###########################################################################################\n",
    "\n",
    "def calibrate_kappa(out_df: pl.DataFrame, eps_seconds: float, gamma: float = 0.49, alpha: float = 0.001) -> float:\n",
    "    \"\"\"\n",
    "    Calibrate kappa via v_n ≈ q_{1-α}(|Δy|):  kappa = q / ε_n^γ.\n",
    "    \"\"\"\n",
    "    base = out_df.filter(pl.col(\"ret\").is_not_null())\n",
    "    q_tail = float(base.select(pl.col(\"ret\").abs().quantile(1 - alpha).alias(\"q\")).item())\n",
    "    kappa_new = q_tail / (eps_seconds ** gamma)\n",
    "    return kappa_new\n",
    "\n",
    "def reconstruct_dejumped_prices(out_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    For each day: price_nojump = exp( logp0 + cumsum(ret_nojump with NULL->0) )\n",
    "    \"\"\"\n",
    "    return (\n",
    "        out_df\n",
    "        .with_columns([\n",
    "            pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "            pl.col(\"ret_nojump\").fill_null(0.0).alias(\"ret_nj0\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(\"ret_nj0\").cum_sum().over(\"date\").alias(\"cs_ret_nj0\"),\n",
    "            pl.first(\"logp\").over(\"date\").alias(\"logp0\"),\n",
    "        ])\n",
    "        .with_columns((pl.col(\"logp0\") + pl.col(\"cs_ret_nj0\")).exp().alias(\"price_nojump\"))\n",
    "        .select([\"timestamp\",\"date\",\"price\",\"price_nojump\",\"ret\",\"ret_nojump\",\"is_jump\",\"v_n\"])\n",
    "        .sort(\"timestamp\")\n",
    "    )\n",
    "\n",
    "def plot_day(out_df: pl.DataFrame, DAY: str | None = None):\n",
    "    pdf = reconstruct_dejumped_prices(out_df)\n",
    "    last_day = pdf.select(pl.max(\"date\")).item()\n",
    "    day = Date.fromisoformat(DAY) if DAY else last_day\n",
    "\n",
    "    day_view = (\n",
    "        pdf.filter(pl.col(\"date\") == pl.lit(day))\n",
    "           .select([\"timestamp\",\"price\",\"price_nojump\",\"ret\",\"ret_nojump\",\"is_jump\",\"v_n\"])\n",
    "           .to_pandas()\n",
    "    )\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.plot(day_view[\"timestamp\"], day_view[\"price\"], label=\"raw price\")\n",
    "    plt.plot(day_view[\"timestamp\"], day_view[\"price_nojump\"], label=\"de-jumped price\")\n",
    "    plt.title(f\"Price vs De-jumped Price — {day}\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Price\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.plot(day_view[\"timestamp\"], day_view[\"ret\"], label=\"raw returns\")\n",
    "    plt.plot(day_view[\"timestamp\"], day_view[\"ret_nojump\"], label=\"returns (jumps removed)\")\n",
    "    if len(day_view):\n",
    "        vn = float(day_view[\"v_n\"].iloc[0])\n",
    "        plt.axhline(vn, linestyle=\"--\"); plt.axhline(-vn, linestyle=\"--\")\n",
    "    if \"is_jump\" in day_view.columns:\n",
    "        m = day_view[\"is_jump\"] == True\n",
    "        if m.any():\n",
    "            plt.scatter(day_view.loc[m,\"timestamp\"], day_view.loc[m,\"ret\"], marker=\"x\")\n",
    "    plt.title(f\"Intraday Returns with ±vₙ — {day}\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Return\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_overview(out_df: pl.DataFrame, df_price: pl.DataFrame):\n",
    "\n",
    "    p_all = df_price.select([\"timestamp\",\"price\"]).to_pandas()\n",
    "    plt.figure(figsize=(11,4)); plt.plot(p_all[\"timestamp\"], p_all[\"price\"])\n",
    "    plt.title(\"Price over time\"); plt.xlabel(\"Time\"); plt.ylabel(\"Price\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "    jump_daily = (\n",
    "        out_df\n",
    "        .select([\n",
    "            pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "            pl.col(\"is_jump\")\n",
    "        ])\n",
    "        .group_by(\"date\")\n",
    "        .agg([\n",
    "            pl.len().alias(\"n_obs\"),\n",
    "            pl.col(\"is_jump\").sum().fill_null(0).cast(pl.Int64).alias(\"n_jumps\")\n",
    "        ])\n",
    "        .sort(\"date\")\n",
    "        .to_pandas()\n",
    "    )\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.bar(jump_daily[\"date\"].astype(str), jump_daily[\"n_jumps\"])\n",
    "    plt.title(\"Number of jumps per day\"); plt.xlabel(\"Date\"); plt.ylabel(\"# jumps\")\n",
    "    plt.xticks(rotation=90); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "    rv_daily = (\n",
    "        out_df\n",
    "        .select([\n",
    "            pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "            (pl.col(\"ret\")**2).alias(\"rv_raw\"),\n",
    "            (pl.col(\"ret_nojump\")**2).alias(\"rv_trunc\")\n",
    "        ])\n",
    "        .group_by(\"date\")\n",
    "        .agg([\n",
    "            pl.sum(\"rv_raw\").alias(\"RV_raw\"),\n",
    "            pl.sum(\"rv_trunc\").alias(\"RV_trunc\"),\n",
    "        ])\n",
    "        .sort(\"date\")\n",
    "        .to_pandas()\n",
    "    )\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.plot(rv_daily[\"date\"].astype(str), rv_daily[\"RV_raw\"], label=\"RV raw\")\n",
    "    plt.plot(rv_daily[\"date\"].astype(str), rv_daily[\"RV_trunc\"], label=\"RV truncated\")\n",
    "    plt.title(\"Daily realized variance: raw vs jump-truncated\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Realized variance\")\n",
    "    plt.xticks(rotation=90); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "df = load_daily_dir_to_polars(\"daily_csv\")\n",
    "print(df.head()); print(df.schema)\n",
    "\n",
    "gamma = 0.49\n",
    "out_df0, clean_df0, v_n0, eps = filter_jumps_paper_threshold(\n",
    "    df, gamma=gamma, kappa=1.0, eps_seconds=None, drop_excluded_days=True\n",
    ")\n",
    "print(f\"ε_n (seconds) inferred = {eps:.6g} | (temporary) v_n = {v_n0:.6g}\")\n",
    "\n",
    "alpha = 0.001  # 0.1% tail\n",
    "kappa_hat = calibrate_kappa(out_df0, eps_seconds=eps, gamma=gamma, alpha=alpha)\n",
    "print(f\"Calibrated kappa @ alpha={alpha:.4f} -> kappa = {kappa_hat:.6g}\")\n",
    "\n",
    "out_df, clean_df, v_n, eps2 = filter_jumps_paper_threshold(\n",
    "    df, gamma=gamma, kappa=kappa_hat, eps_seconds=eps, drop_excluded_days=True\n",
    ")\n",
    "print(f\"Final v_n = {v_n:.6g} (using calibrated kappa)\")\n",
    "\n",
    "\n",
    "sanity_checks(out_df, v_n=v_n, eps=eps2)\n",
    "plot_overview(out_df, df_price=df)\n",
    "plot_day(out_df)            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b30abf",
   "metadata": {},
   "source": [
    "$$\n",
    "Z_t^{n,j}=\n",
    "\\begin{cases}\n",
    "\\bigl(k_n^{(j)}\\delta_n\\bigr)^{\\frac12-2H}\n",
    "\\Bigl(\\widehat V_{t}^{\\,n,\\ell_j,k_n^{(j)}}\n",
    "      - \\bigl(k_n^{(j)}\\delta_n\\bigr)^{2H-1} V_t^{\\ell_j}\\Bigr),\n",
    "& \\text{if }\\ell_j\\ge 2,\\\\[6pt]\n",
    "\\bigl(k_n^{(j)}\\delta_n\\bigr)^{\\frac12-2H}\n",
    "\\Bigl(\\widehat V_{t}^{\\,n,0,k_n^{(j)}}\n",
    "      + 2\\,\\widehat V_{t}^{\\,n,1,k_n^{(j)}}\n",
    "      - \\bigl(k_n^{(j)}\\delta_n\\bigr)^{2H-1}\\bigl(V_t^{0}+2V_t^{1}\\bigr)\\Bigr),\n",
    "& \\text{if }\\ell_j=1.\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85583a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Dict, List\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def infer_eps_dimless(df: pl.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Empirical ε_n as '1 / median(#returns per day)' so k_n ε_n is dimensionless.\n",
    "    Works on out_df that already contains 'ret_nojump'.\n",
    "    \"\"\"\n",
    "    tmp = (\n",
    "        df.with_columns(pl.col(\"timestamp\").dt.date().alias(\"date\"))\n",
    "          .with_columns(pl.col(\"ret_nojump\").is_not_null().cast(pl.Int32).alias(\"has_ret\"))\n",
    "          .group_by(\"date\").agg(pl.col(\"has_ret\").sum().alias(\"n_increments\"))\n",
    "          .filter(pl.col(\"n_increments\") > 0)\n",
    "    )\n",
    "    n_bar = float(tmp.select(pl.median(\"n_increments\")).item())\n",
    "    eps_n = 1.0 / n_bar\n",
    "    return eps_n, n_bar  \n",
    "\n",
    "def choose_k_list(eps_n: float, kappas: Sequence[float]) -> List[int]:\n",
    "    \"\"\"\n",
    "    k_n ≈ κ * ε_n^{-1/2} * log(ε_n^{-1}). See the paper's choice for windows. :contentReference[oaicite:4]{index=4}\n",
    "    \"\"\"\n",
    "    s = (eps_n**-0.5) * np.log(1.0/eps_n)\n",
    "    ks = [max(2, int(round(kappa * s))) for kappa in kappas]\n",
    "    return ks\n",
    "\n",
    "# φ_ρ(H) from eq. (6) (5-point stencil); deterministic in H.\n",
    "def phi_rho(H: float, rho: int) -> float:\n",
    "    p = 2.0*H + 2.0\n",
    "    f = lambda x: abs(float(x))**p\n",
    "    num = (f(rho+2) - 4.0*f(rho+1) + 6.0*f(rho) - 4.0*f(rho-1) + f(rho-2))\n",
    "    den = 2.0*(2.0*H+1.0)*(2.0*H+2.0)\n",
    "    return num/den\n",
    "\n",
    "# For the ρ=1 coordinate, the CLT/gmm uses a combination.\n",
    "def phi_combo(H: float, rho: int) -> float:\n",
    "    if rho == 1:\n",
    "        return phi_rho(H, 0) + 2.0*phi_rho(H, 1)\n",
    "    else:\n",
    "        return phi_rho(H, rho)\n",
    "\n",
    "##########################################################################################\n",
    "################## \\tilde V_{n,ρ,k}(t) ######################\n",
    "##########################################################################################\n",
    "\n",
    "def compute_pV_components(\n",
    "    out_df: pl.DataFrame,\n",
    "    eps_n: float,\n",
    "    ks: Sequence[int],\n",
    "    rhos: Sequence[int],\n",
    "    ret_col: str = \"ret_nojump\",\n",
    ") -> Dict[Tuple[int,int], float]:\n",
    "    \"\"\"\n",
    "    Compute \\tilde V_{n,ρ,k}(t) = (1/k) * sum_i (Δ^k c_i) (Δ^k c_{i+ρk}),\n",
    "    with c_i = (1/(k ε_n)) ∑_{j=i}^{i+k-1} (ret_j^2). (Jump-truncated returns.)\n",
    "    No volatility-jump truncation (paper shows it can be removed). :contentReference[oaicite:7]{index=7}\n",
    "    Returns a dict keyed by (k, ρ).\n",
    "    \"\"\"\n",
    "    df = (out_df\n",
    "          .select([\"timestamp\", ret_col])\n",
    "          .with_columns([\n",
    "              pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "              pl.coalesce([pl.col(ret_col), pl.lit(0.0)]).alias(\"r\")  \n",
    "          ])\n",
    "          .sort(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    results: Dict[Tuple[int,int], float] = {}\n",
    "\n",
    "    for k in ks:\n",
    "        c_col = f\"c_k{k}\"\n",
    "        d_col = f\"dC_k{k}\"\n",
    "        df_k = df.with_columns([\n",
    "            # rolling sum of r^2 within day; scale to spot variance estimate\n",
    "            ((pl.col(\"r\")**2).rolling_sum(window_size=k, min_periods=k).over(\"date\") / (k*eps_n)).alias(c_col),\n",
    "        ]).with_columns([\n",
    "            # forward difference Δ^k c_i = c_{i+k} - c_i, within day\n",
    "            (pl.col(c_col).shift(-k).over(\"date\") - pl.col(c_col).over(\"date\")).alias(d_col)\n",
    "        ])\n",
    "\n",
    "        for rho in rhos:\n",
    "            tri_col = f\"tri_r{rho}_k{k}\"\n",
    "            df_tri = df_k.with_columns([\n",
    "                # product (Δ^k c_i) (Δ^k c_{i+ρk}), all within the same day via 'over'\n",
    "                (pl.col(d_col).over(\"date\") * pl.col(d_col).shift(rho*k).over(\"date\")).alias(tri_col)\n",
    "            ])\n",
    "            # (1/k) * sum over valid i across the whole sample\n",
    "            val = float(df_tri.select(pl.col(tri_col).drop_nulls().sum()).item()) / k\n",
    "            results[(k, rho)] = val\n",
    "\n",
    "    return results\n",
    "\n",
    "################## Assemble rV_n(t) entries and estimate (H, R_t) by 1D optimization ##################\n",
    "\n",
    "@dataclass\n",
    "class GMMSetup:\n",
    "    kappas: List[float]\n",
    "    rhos:   List[int]   \n",
    "\n",
    "@dataclass\n",
    "class GMMResult:\n",
    "    H_hat: float\n",
    "    R_hat: float\n",
    "    ks: List[int]\n",
    "    eps_n: float\n",
    "    kn_epsn: List[float]           # k_n ε_n for each coordinate\n",
    "    rV_vec: np.ndarray             # data vector\n",
    "    phi_vec_hat: np.ndarray        # φ(H_hat) vector\n",
    "    Z_vec: np.ndarray              # Z_t^{n,j} at the optimum\n",
    "\n",
    "def build_rV_vector(pV: Dict[Tuple[int,int], float], ks: List[int], rhos: List[int]) -> np.ndarray:\n",
    "    vals = []\n",
    "    for k, rho in zip(ks, rhos):\n",
    "        if rho == 1:\n",
    "            # rV^{(j)} = \\tilde V_{n,0,k} + 2 \\tilde V_{n,1,k}. \n",
    "            vals.append(pV[(k,0)] + 2.0 * pV[(k,1)])\n",
    "        else:\n",
    "            vals.append(pV[(k,rho)])\n",
    "    return np.asarray(vals, dtype=float)\n",
    "\n",
    "def phi_vector(H: float, rhos: List[int]) -> np.ndarray:\n",
    "    return np.asarray([phi_combo(H, rho) for rho in rhos], dtype=float)\n",
    "\n",
    "def closed_form_R_hat(W: np.ndarray, rV: np.ndarray, phiH: np.ndarray) -> float:\n",
    "    num = phiH.T @ W @ rV\n",
    "    den = phiH.T @ W @ phiH\n",
    "    return float(num/den)\n",
    "\n",
    "def minimize_over_H(\n",
    "    rV: np.ndarray,\n",
    "    rhos: List[int],\n",
    "    W: np.ndarray | None = None,\n",
    "    H_grid: np.ndarray | None = None,\n",
    ") -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Minimize || W^{1/2}( rV - φ(H) R(H) ) ||^2 over H in (0, 1/2).\n",
    "    R(H) is in closed form. Weight W defaults to Identity.\n",
    "    Coarse-to-fine 1-D search (no SciPy needed).\n",
    "    \"\"\"\n",
    "    m = len(rV)\n",
    "    W = np.eye(m) if W is None else W\n",
    "    if H_grid is None:\n",
    "        H_grid = np.linspace(0.05, 0.49, 451)  # fine grid\n",
    "\n",
    "    best = (np.inf, None, None)  # (obj, H, R)\n",
    "    for H in H_grid:\n",
    "        phiH = phi_vector(H, rhos)\n",
    "        R = closed_form_R_hat(W, rV, phiH)\n",
    "        resid = rV - phiH * R\n",
    "        obj = resid.T @ W @ resid\n",
    "        if obj < best[0]:\n",
    "            best = (obj, H, R)\n",
    "\n",
    "    H_hat, R_hat = float(best[1]), float(best[2])\n",
    "    return H_hat, R_hat, phi_vector(H_hat, rhos)\n",
    "\n",
    "def compute_Z_vec(\n",
    "    rV: np.ndarray, ks: List[int], eps_n: float, rhos: List[int], H_hat: float, R_hat: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Z^{n,j}_t after plugging (H_hat, R_hat).\n",
    "    Uses (k ε)^{1/2 - 2H} [ rV_j - φ_j(H) R_hat ]. See eq. (9). :contentReference[oaicite:9]{index=9}\n",
    "    \"\"\"\n",
    "    phiH = phi_vector(H_hat, rhos)\n",
    "    scales = np.asarray([(k*eps_n)**(0.5 - 2.0*H_hat) for k in ks])\n",
    "    return scales * (rV - phiH * R_hat)\n",
    "\n",
    "def estimate_H_R_and_Z(\n",
    "    out_df: pl.DataFrame,\n",
    "    setup: GMMSetup,\n",
    "    ret_col: str = \"ret_nojump\",\n",
    "    W: np.ndarray | None = None,\n",
    ") -> GMMResult:\n",
    "    # ε_n and k list\n",
    "    eps_n, _ = infer_eps_dimless(out_df)\n",
    "    ks = choose_k_list(eps_n, setup.kappas)\n",
    "\n",
    "    # need pV_{ρ,k} for all ρ we might combine\n",
    "    needed_rhos = sorted(set([0 if r==1 else r for r in setup.rhos] + [1 if 1 in setup.rhos else -1]))\n",
    "    if -1 in needed_rhos: needed_rhos.remove(-1)\n",
    "    pV = compute_pV_components(out_df, eps_n, ks, needed_rhos, ret_col=ret_col)\n",
    "\n",
    "    # data vector\n",
    "    rV_vec = build_rV_vector(pV, ks, setup.rhos)\n",
    "\n",
    "    # estimate\n",
    "    H_hat, R_hat, phi_hat = minimize_over_H(rV_vec, setup.rhos, W=W)\n",
    "\n",
    "    # Z and k_n ε_n\n",
    "    Z_vec = compute_Z_vec(rV_vec, ks, eps_n, setup.rhos, H_hat, R_hat)\n",
    "    kn_epsn = [k*eps_n for k in ks]\n",
    "\n",
    "    return GMMResult(\n",
    "        H_hat=H_hat, R_hat=R_hat, ks=ks, eps_n=eps_n,\n",
    "        kn_epsn=kn_epsn, rV_vec=rV_vec, phi_vec_hat=phi_hat, Z_vec=Z_vec\n",
    "    )\n",
    "\n",
    "\n",
    "################### example ######################################################\n",
    "setup = GMMSetup(\n",
    "    kappas=[0.9, 1.2, 1.6, 2.0],   \n",
    "    rhos   =[0,   1,   2,   5]     \n",
    ")\n",
    "\n",
    "res = estimate_H_R_and_Z(out_df, setup, ret_col=\"ret_nojump\")\n",
    "\n",
    "print(\"\\n=== Data-driven window sizes ===\")\n",
    "for j,(k,keps) in enumerate(zip(res.ks, res.kn_epsn), start=1):\n",
    "    print(f\"j={j}: k_n={k:5d},  k_n*ε_n={keps:.5f}\")\n",
    "\n",
    "print(f\"\\nEstimated H: {res.H_hat:.4f}\")\n",
    "print(f\"Estimated R_t = ∫_0^t |η|^2 ds: {res.R_hat:.6g}\")\n",
    "\n",
    "print(\"\\nVector rV_n(t):\\n\", res.rV_vec)\n",
    "print(\"\\nφ(H_hat):\\n\", res.phi_vec_hat)\n",
    "print(\"\\nZ_t^{n,j} at optimum (should look ≈ mean-zero across coords under CLT scaling):\\n\", res.Z_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# RESULT ################################################################\n",
    "########################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats  \n",
    "\n",
    "\n",
    "def stem_compat(ax, x, y, **kw):\n",
    "    try:\n",
    "        return ax.stem(x, y, use_line_collection=True, **kw)  # newer MPL\n",
    "    except TypeError:\n",
    "        return ax.stem(x, y, **kw)                            # older MPL\n",
    "\n",
    "def _phi_rho_fallback(H: float, rho: int) -> float:\n",
    "    \"\"\"\n",
    "    5-point FD kernel for φ_ρ(H).\n",
    "    \"\"\"\n",
    "    p = 2.0 * H + 2.0\n",
    "    f = lambda x: abs(float(x)) ** p\n",
    "    num = (f(rho + 2) - 4.0 * f(rho + 1) + 6.0 * f(rho) - 4.0 * f(rho - 1) + f(rho - 2))\n",
    "    den = 2.0 * (2.0 * H + 1.0) * (2.0 * H + 2.0)\n",
    "    return num / den\n",
    "\n",
    "def phi_combo_fallback(H: float, rho: int) -> float:\n",
    "    return _phi_rho_fallback(H, 0) + 2.0 * _phi_rho_fallback(H, 1) if rho == 1 else _phi_rho_fallback(H, rho)\n",
    "\n",
    "def phi_vector_fallback(H: float, rhos):\n",
    "    return np.asarray([phi_combo_fallback(H, r) for r in rhos], dtype=float)\n",
    "\n",
    "def R_hat_closed_form_fallback(W: np.ndarray, rV: np.ndarray, phiH: np.ndarray) -> float:\n",
    "    num = phiH.T @ W @ rV\n",
    "    den = phiH.T @ W @ phiH\n",
    "    return float(num / den)\n",
    "\n",
    "def objective_for_H_fallback(H: float, rV: np.ndarray, rhos, W: np.ndarray) -> float:\n",
    "    phiH = phi_vector_fallback(H, rhos)\n",
    "    Rhat = R_hat_closed_form_fallback(W, rV, phiH)\n",
    "    resid = rV - phiH * Rhat\n",
    "    return float(resid.T @ W @ resid)\n",
    "\n",
    "def plot_gmm_results(res, setup=None, *, save=False, prefix=\"gmm_plots\"):\n",
    "    \"\"\"\n",
    "    Plots:\n",
    "      1) window sizes k_n and scaled k_n·mesh (mesh = δ_n or ε_n),\n",
    "      2) data rV vs model φ(Ĥ)·R̂_t,\n",
    "      3) residuals and studentized Z,\n",
    "      4) QQ-plot of Z vs N(0,1),\n",
    "      5) objective profile around Ĥ,\n",
    "      6) φ_ρ(H) curves with Ĥ marker.\n",
    "    \"\"\"\n",
    "    Hhat = float(res.H_hat)\n",
    "    Rhat = float(res.R_hat)\n",
    "    ks   = np.asarray(res.ks, dtype=int)\n",
    "\n",
    "    if hasattr(res, \"kn_delta\"):              \n",
    "        kd = np.asarray(res.kn_delta, dtype=float)\n",
    "        mesh = float(res.delta_n)\n",
    "        mesh_name = \"delta\"                   \n",
    "        mesh_math = r\"\\delta_n\"               \n",
    "    else:                                     \n",
    "        kd = np.asarray(res.kn_epsn, dtype=float)\n",
    "        mesh = float(res.eps_n)\n",
    "        mesh_name = \"epsilon\"\n",
    "        mesh_math = r\"\\varepsilon_n\"\n",
    "\n",
    "    rV   = np.asarray(res.rV_vec, dtype=float)\n",
    "    phiH = np.asarray(res.phi_vec_hat, dtype=float)\n",
    "    Z    = np.asarray(res.Z_vec, dtype=float)\n",
    "\n",
    "    # rhos for annotations/curves\n",
    "    if setup is not None and hasattr(setup, \"rhos\"):\n",
    "        rhos = list(setup.rhos)\n",
    "    else:\n",
    "        rhos = list(range(len(rV)))\n",
    "\n",
    "    pred  = phiH * Rhat\n",
    "    resid = rV - pred\n",
    "    m     = len(rV)\n",
    "    js    = np.arange(1, m + 1)\n",
    "\n",
    "    #################################################################\n",
    "    ################## Acquire φ ####################################\n",
    "    #################################################################\n",
    "    phi_combo_fn = globals().get(\"phi_combo\", None) or phi_combo_fallback\n",
    "    objective_H_fn = globals().get(\"objective_for_H\", None) or objective_for_H_fallback\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax[0].bar(js, ks)\n",
    "    ax[0].set_title(\"Window sizes $k_n$\")\n",
    "    ax[0].set_xlabel(\"Coordinate $j$\")\n",
    "    ax[0].set_ylabel(\"$k_n$\")\n",
    "    ax[0].set_xticks(js)\n",
    "\n",
    "    ax[1].bar(js, kd)\n",
    "    ax[1].set_title(fr\"Scaled windows $k_n\\,{mesh_math}$\")\n",
    "    ax[1].set_xlabel(\"Coordinate $j$\")\n",
    "    ax[1].set_ylabel(fr\"$k_n\\,{mesh_math}$\")\n",
    "    ax[1].set_xticks(js)\n",
    "    for i, v in enumerate(kd, 1):\n",
    "        ax[1].text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_windows.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    width = 0.38\n",
    "    fig, ax = plt.subplots(figsize=(11, 4))\n",
    "    ax.bar(js - width/2, rV, width=width, label=r\"$rV_n^{(j)}(t)$\")\n",
    "    ax.bar(js + width/2, pred, width=width, label=rf\"$\\phi_j(\\widehat{{H}})\\,\\widehat{{R}}_t$\")\n",
    "    ax.set_title(r\"Data vs model: $rV_n^{(j)}(t)$ vs $\\phi_j(\\widehat{H})\\widehat{R}_t$\")\n",
    "    ax.set_xlabel(\"Coordinate $j$\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_xticks(js)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_fit.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(11, 6), sharex=True)\n",
    "\n",
    "    stem_compat(ax[0], js, resid, basefmt=\" \", linefmt=\"-\", markerfmt=\"o\")\n",
    "    ax[0].set_title(r\"Raw residuals $rV_n^{(j)}(t) - \\phi_j(\\widehat{H})\\widehat{R}_t$\")\n",
    "    ax[0].set_ylabel(\"Residual\")\n",
    "\n",
    "    stem_compat(ax[1], js, Z, basefmt=\" \", linefmt=\"-\", markerfmt=\"o\")\n",
    "    ax[1].hlines([0], js.min()-0.5, js.max()+0.5, linestyles=\"dashed\")\n",
    "    ax[1].set_title(\n",
    "        rf\"Studentized $Z_t^{{n,j}}=(k_n\\,{mesh_math})^{{\\frac{{1}}{{2}}-2\\widehat{{H}}}}\"\n",
    "        r\"(rV_n^{(j)}-\\phi_j(\\widehat{H})\\widehat{R}_t)$\"\n",
    "    )\n",
    "    ax[1].set_xlabel(\"Coordinate $j$\")\n",
    "    ax[1].set_ylabel(r\"$Z_t^{n,j}$\")\n",
    "    ax[1].set_xticks(js)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_residuals_Z.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    stats.probplot(Z, dist=\"norm\", plot=plt)\n",
    "    plt.title(r\"QQ plot of $Z_t^{n,j}$ vs $\\mathcal{N}(0,1)$\")\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_qq.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    H_lo = max(0.05, Hhat - 0.15)\n",
    "    H_hi = min(0.49, Hhat + 0.15)\n",
    "    H_grid = np.linspace(H_lo, H_hi, 401)\n",
    "    obj = [objective_H_fn(h, rV, rhos, np.eye(m)) for h in H_grid]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(H_grid, obj)\n",
    "    plt.axvline(Hhat, linestyle=\"--\")\n",
    "    plt.title(r\"Objective profile $\\|rV_n(t)-\\phi(H)R(H)\\|^2$ over $H$\")\n",
    "    plt.xlabel(r\"$H$\")\n",
    "    plt.ylabel(\"Objective\")\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_objective.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    Hs = np.linspace(0.05, 0.49, 300)\n",
    "    Phi_curves = np.vstack([[phi_combo_fn(h, r) for h in Hs] for r in rhos])\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, r in enumerate(rhos):\n",
    "        plt.plot(Hs, Phi_curves[i, :], label=rf\"$\\phi_{{\\rho={r}}}(H)$\")\n",
    "    vals_at_hat = [phi_combo_fn(Hhat, r) for r in rhos]\n",
    "    plt.scatter([Hhat] * len(rhos), vals_at_hat)\n",
    "    plt.title(r\"Deterministic kernels $\\phi_\\rho(H)$ for used lags\")\n",
    "    plt.xlabel(r\"$H$\")\n",
    "    plt.ylabel(r\"$\\phi_\\rho(H)$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(f\"{prefix}_phi_curves.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython()  \n",
    "    import matplotlib_inline.backend_inline as _bi\n",
    "    _bi.set_matplotlib_formats('retina')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if \"res\" not in globals():\n",
    "    raise NameError(\"`res` is not defined. Run the estimator first to create `res`.\")\n",
    "_plot_setup = setup if \"setup\" in globals() else None\n",
    "\n",
    "plot_gmm_results(res, _plot_setup, save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc4be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
